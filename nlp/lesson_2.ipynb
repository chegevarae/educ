{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "associate-moore",
   "metadata": {},
   "source": [
    "# ДЗ 2. Создание признакового пространства"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "whole-israel",
   "metadata": {},
   "source": [
    "Продолжим обработку данных с Твиттера."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "joined-spanking",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn import model_selection, preprocessing, linear_model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, median_absolute_error, r2_score\n",
    "from sklearn import decomposition\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "martial-parker",
   "metadata": {},
   "source": [
    "### Загрузка и подготовка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "outdoor-block",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "      <th>result</th>\n",
       "      <th>token</th>\n",
       "      <th>token_filtered</th>\n",
       "      <th>tweet_stemmed</th>\n",
       "      <th>tweet_lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>@user when a father is dysfunctional and is s...</td>\n",
       "      <td>when father is dysfunctional and is so selfish...</td>\n",
       "      <td>[when, father, is, dysfunctional, and, is, so,...</td>\n",
       "      <td>[father, dysfunctional, selfish, drags, kids, ...</td>\n",
       "      <td>[father, dysfunct, selfish, drag, kid, dysfunc...</td>\n",
       "      <td>[father, dysfunctional, selfish, drag, kid, dy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
       "      <td>thanks for lyft credit cannot use cause they d...</td>\n",
       "      <td>[thanks, for, lyft, credit, can, not, use, cau...</td>\n",
       "      <td>[thanks, lyft, credit, use, cause, offer, whee...</td>\n",
       "      <td>[thank, lyft, credit, use, caus, offer, wheelc...</td>\n",
       "      <td>[thanks, lyft, credit, use, cause, offer, whee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "      <td>bihday your majesty</td>\n",
       "      <td>[bihday, your, majesty]</td>\n",
       "      <td>[bihday, majesty]</td>\n",
       "      <td>[bihday, majesti]</td>\n",
       "      <td>[bihday, majesty]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>#model   i love u take with u all the time in ...</td>\n",
       "      <td>model love you take with you all the time in ur</td>\n",
       "      <td>[model, love, you, take, with, you, all, the, ...</td>\n",
       "      <td>[model, love, take, time, ur]</td>\n",
       "      <td>[model, love, take, time, ur]</td>\n",
       "      <td>[model, love, take, time, ur]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "      <td>factsguide society now motivation</td>\n",
       "      <td>[factsguide, society, now, motivation]</td>\n",
       "      <td>[factsguide, society, motivation]</td>\n",
       "      <td>[factsguid, societi, motiv]</td>\n",
       "      <td>[factsguide, society, motivation]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label                                              tweet  \\\n",
       "0   1    0.0   @user when a father is dysfunctional and is s...   \n",
       "1   2    0.0  @user @user thanks for #lyft credit i can't us...   \n",
       "2   3    0.0                                bihday your majesty   \n",
       "3   4    0.0  #model   i love u take with u all the time in ...   \n",
       "4   5    0.0             factsguide: society now    #motivation   \n",
       "\n",
       "                                              result  \\\n",
       "0  when father is dysfunctional and is so selfish...   \n",
       "1  thanks for lyft credit cannot use cause they d...   \n",
       "2                                bihday your majesty   \n",
       "3    model love you take with you all the time in ur   \n",
       "4                  factsguide society now motivation   \n",
       "\n",
       "                                               token  \\\n",
       "0  [when, father, is, dysfunctional, and, is, so,...   \n",
       "1  [thanks, for, lyft, credit, can, not, use, cau...   \n",
       "2                            [bihday, your, majesty]   \n",
       "3  [model, love, you, take, with, you, all, the, ...   \n",
       "4             [factsguide, society, now, motivation]   \n",
       "\n",
       "                                      token_filtered  \\\n",
       "0  [father, dysfunctional, selfish, drags, kids, ...   \n",
       "1  [thanks, lyft, credit, use, cause, offer, whee...   \n",
       "2                                  [bihday, majesty]   \n",
       "3                      [model, love, take, time, ur]   \n",
       "4                  [factsguide, society, motivation]   \n",
       "\n",
       "                                       tweet_stemmed  \\\n",
       "0  [father, dysfunct, selfish, drag, kid, dysfunc...   \n",
       "1  [thank, lyft, credit, use, caus, offer, wheelc...   \n",
       "2                                  [bihday, majesti]   \n",
       "3                      [model, love, take, time, ur]   \n",
       "4                        [factsguid, societi, motiv]   \n",
       "\n",
       "                                    tweet_lemmatized  \n",
       "0  [father, dysfunctional, selfish, drag, kid, dy...  \n",
       "1  [thanks, lyft, credit, use, cause, offer, whee...  \n",
       "2                                  [bihday, majesty]  \n",
       "3                      [model, love, take, time, ur]  \n",
       "4                  [factsguide, society, motivation]  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combine_df = pd.read_pickle(\"data/combine_df.pkl\")\n",
    "combine_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "demonstrated-coral",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Преобразуем данные и выделим их в отдельный датафрейм\n",
    "combine_df['tweet_stemmed'] = combine_df['tweet_stemmed'].apply(lambda tokens: ' '.join(tokens))\n",
    "combine_df['tweet_lemmatized'] = combine_df['tweet_lemmatized'].apply(lambda tokens: ' '.join(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "american-music",
   "metadata": {},
   "source": [
    "### Мешок слов с помощью CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aquatic-leave",
   "metadata": {},
   "source": [
    "**Создайте мешок слов с помощью sklearn.feature_extraction.text.CountVectorizer.fit_transform()**  \n",
    "\n",
    "Применим его к 'tweet_stemmed' и 'tweet_lemmatized' отдельно.  \n",
    "\n",
    "- Игнорируем слова, частота которых в документе строго превышает порог 0.9 с помощью max_df.  \n",
    "- Ограничим количество слов, попадающий в мешок, с помощью max_features = 1000.  \n",
    "- Исключим стоп-слова с помощью stop_words='english'.  \n",
    "- Отобразим Bag-of-Words модель как DataFrame. columns необходимо извлечь с помощью CountVectorizer.get_feature_names().  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "incident-colombia",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(ngram_range=(1, 1), \n",
    "                                   analyzer='word', \n",
    "                                   binary=False, \n",
    "                                   tokenizer=str.split, \n",
    "                                   stop_words=\"english\", \n",
    "                                   max_df=0.9, \n",
    "                                   max_features=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "satellite-passion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем the Bag-of-Words модель для tweet_stemmed\n",
    "bag_of_words_stemmed = count_vectorizer.fit_transform(combine_df['tweet_stemmed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "excellent-boating",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abl</th>\n",
       "      <th>absolut</th>\n",
       "      <th>accept</th>\n",
       "      <th>account</th>\n",
       "      <th>act</th>\n",
       "      <th>action</th>\n",
       "      <th>actor</th>\n",
       "      <th>actual</th>\n",
       "      <th>ad</th>\n",
       "      <th>adapt</th>\n",
       "      <th>...</th>\n",
       "      <th>yeah</th>\n",
       "      <th>year</th>\n",
       "      <th>yesterday</th>\n",
       "      <th>yo</th>\n",
       "      <th>yoga</th>\n",
       "      <th>york</th>\n",
       "      <th>young</th>\n",
       "      <th>youtub</th>\n",
       "      <th>yr</th>\n",
       "      <th>yummi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   abl  absolut  accept  account  act  action  actor  actual  ad  adapt  ...  \\\n",
       "0    0        0       0        0    0       0      0       0   0      0  ...   \n",
       "1    0        0       0        0    0       0      0       0   0      0  ...   \n",
       "2    0        0       0        0    0       0      0       0   0      0  ...   \n",
       "3    0        0       0        0    0       0      0       0   0      0  ...   \n",
       "4    0        0       0        0    0       0      0       0   0      0  ...   \n",
       "\n",
       "   yeah  year  yesterday  yo  yoga  york  young  youtub  yr  yummi  \n",
       "0     0     0          0   0     0     0      0       0   0      0  \n",
       "1     0     0          0   0     0     0      0       0   0      0  \n",
       "2     0     0          0   0     0     0      0       0   0      0  \n",
       "3     0     0          0   0     0     0      0       0   0      0  \n",
       "4     0     0          0   0     0     0      0       0   0      0  \n",
       "\n",
       "[5 rows x 1000 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Отобразим Bag-of-Words модель как DataFrame\n",
    "feature_names = count_vectorizer.get_feature_names()\n",
    "stemmed_count = pd.DataFrame(bag_of_words_stemmed.toarray(), columns = feature_names)\n",
    "stemmed_count.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "functional-fellowship",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем the Bag-of-Words модель для tweet_lemmatized\n",
    "bag_of_words_lemmatized = count_vectorizer.fit_transform(combine_df['tweet_lemmatized'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "adopted-operation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>able</th>\n",
       "      <th>absolutely</th>\n",
       "      <th>account</th>\n",
       "      <th>act</th>\n",
       "      <th>action</th>\n",
       "      <th>actor</th>\n",
       "      <th>actually</th>\n",
       "      <th>adapt</th>\n",
       "      <th>add</th>\n",
       "      <th>adventure</th>\n",
       "      <th>...</th>\n",
       "      <th>year</th>\n",
       "      <th>yes</th>\n",
       "      <th>yesterday</th>\n",
       "      <th>yo</th>\n",
       "      <th>yoga</th>\n",
       "      <th>york</th>\n",
       "      <th>young</th>\n",
       "      <th>youtube</th>\n",
       "      <th>yr</th>\n",
       "      <th>yummy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   able  absolutely  account  act  action  actor  actually  adapt  add  \\\n",
       "0     0           0        0    0       0      0         0      0    0   \n",
       "1     0           0        0    0       0      0         0      0    0   \n",
       "2     0           0        0    0       0      0         0      0    0   \n",
       "3     0           0        0    0       0      0         0      0    0   \n",
       "4     0           0        0    0       0      0         0      0    0   \n",
       "\n",
       "   adventure  ...  year  yes  yesterday  yo  yoga  york  young  youtube  yr  \\\n",
       "0          0  ...     0    0          0   0     0     0      0        0   0   \n",
       "1          0  ...     0    0          0   0     0     0      0        0   0   \n",
       "2          0  ...     0    0          0   0     0     0      0        0   0   \n",
       "3          0  ...     0    0          0   0     0     0      0        0   0   \n",
       "4          0  ...     0    0          0   0     0     0      0        0   0   \n",
       "\n",
       "   yummy  \n",
       "0      0  \n",
       "1      0  \n",
       "2      0  \n",
       "3      0  \n",
       "4      0  \n",
       "\n",
       "[5 rows x 1000 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Отобразим Bag-of-Words модель как DataFrame\n",
    "feature_names = count_vectorizer.get_feature_names()\n",
    "lemmatized_count = pd.DataFrame(bag_of_words_lemmatized.toarray(), columns = feature_names)\n",
    "lemmatized_count.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regulated-tumor",
   "metadata": {},
   "source": [
    "### Мешок слов с помощью TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imported-lighter",
   "metadata": {},
   "source": [
    "**Создайте мешок слов с помощью sklearn.feature_extraction.text.TfidfVectorizer.fit_transform()**\n",
    "\n",
    "Применим его к 'tweet_stemmed' и 'tweet_lemmatized' отдельно.  \n",
    "\n",
    "- Игнорируем слова, частота которых в документе строго превышает порог 0.9 с помощью max_df.  \n",
    "- Ограничим количество слов, попадающий в мешок, с помощью max_features = 1000.  \n",
    "- Исключим стоп-слова с помощью stop_words='english'.  \n",
    "- Отобразим Bag-of-Words модель как DataFrame. columns необходимо извлечь с помощью TfidfVectorizer.get_feature_names().  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "pacific-tracker",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 1), \n",
    "                                   analyzer='word', \n",
    "                                   binary=False, \n",
    "                                   tokenizer=str.split, \n",
    "                                   stop_words='english', \n",
    "                                   max_df=0.9, \n",
    "                                   max_features=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "white-bowling",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем the Bag-of-Words модель для tweet_stemmed\n",
    "bag_of_words_stemmed = tfidf_vectorizer.fit_transform(combine_df['tweet_stemmed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "affected-vulnerability",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abl</th>\n",
       "      <th>absolut</th>\n",
       "      <th>accept</th>\n",
       "      <th>account</th>\n",
       "      <th>act</th>\n",
       "      <th>action</th>\n",
       "      <th>actor</th>\n",
       "      <th>actual</th>\n",
       "      <th>ad</th>\n",
       "      <th>adapt</th>\n",
       "      <th>...</th>\n",
       "      <th>yeah</th>\n",
       "      <th>year</th>\n",
       "      <th>yesterday</th>\n",
       "      <th>yo</th>\n",
       "      <th>yoga</th>\n",
       "      <th>york</th>\n",
       "      <th>young</th>\n",
       "      <th>youtub</th>\n",
       "      <th>yr</th>\n",
       "      <th>yummi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   abl  absolut  accept  account  act  action  actor  actual   ad  adapt  ...  \\\n",
       "0  0.0      0.0     0.0      0.0  0.0     0.0    0.0     0.0  0.0    0.0  ...   \n",
       "1  0.0      0.0     0.0      0.0  0.0     0.0    0.0     0.0  0.0    0.0  ...   \n",
       "2  0.0      0.0     0.0      0.0  0.0     0.0    0.0     0.0  0.0    0.0  ...   \n",
       "3  0.0      0.0     0.0      0.0  0.0     0.0    0.0     0.0  0.0    0.0  ...   \n",
       "4  0.0      0.0     0.0      0.0  0.0     0.0    0.0     0.0  0.0    0.0  ...   \n",
       "\n",
       "   yeah  year  yesterday   yo  yoga  york  young  youtub   yr  yummi  \n",
       "0   0.0   0.0        0.0  0.0   0.0   0.0    0.0     0.0  0.0    0.0  \n",
       "1   0.0   0.0        0.0  0.0   0.0   0.0    0.0     0.0  0.0    0.0  \n",
       "2   0.0   0.0        0.0  0.0   0.0   0.0    0.0     0.0  0.0    0.0  \n",
       "3   0.0   0.0        0.0  0.0   0.0   0.0    0.0     0.0  0.0    0.0  \n",
       "4   0.0   0.0        0.0  0.0   0.0   0.0    0.0     0.0  0.0    0.0  \n",
       "\n",
       "[5 rows x 1000 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Отобразим Bag-of-Words модель как DataFrame\n",
    "feature_names = tfidf_vectorizer.get_feature_names()\n",
    "stemmed_tfidf = pd.DataFrame(bag_of_words_stemmed.toarray(), columns=feature_names)\n",
    "stemmed_tfidf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "canadian-shipping",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем the Bag-of-Words модель для tweet_lemmatized\n",
    "bag_of_words_lemmatized = tfidf_vectorizer.fit_transform(combine_df['tweet_lemmatized'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "arabic-austin",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>able</th>\n",
       "      <th>absolutely</th>\n",
       "      <th>account</th>\n",
       "      <th>act</th>\n",
       "      <th>action</th>\n",
       "      <th>actor</th>\n",
       "      <th>actually</th>\n",
       "      <th>adapt</th>\n",
       "      <th>add</th>\n",
       "      <th>adventure</th>\n",
       "      <th>...</th>\n",
       "      <th>year</th>\n",
       "      <th>yes</th>\n",
       "      <th>yesterday</th>\n",
       "      <th>yo</th>\n",
       "      <th>yoga</th>\n",
       "      <th>york</th>\n",
       "      <th>young</th>\n",
       "      <th>youtube</th>\n",
       "      <th>yr</th>\n",
       "      <th>yummy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   able  absolutely  account  act  action  actor  actually  adapt  add  \\\n",
       "0   0.0         0.0      0.0  0.0     0.0    0.0       0.0    0.0  0.0   \n",
       "1   0.0         0.0      0.0  0.0     0.0    0.0       0.0    0.0  0.0   \n",
       "2   0.0         0.0      0.0  0.0     0.0    0.0       0.0    0.0  0.0   \n",
       "3   0.0         0.0      0.0  0.0     0.0    0.0       0.0    0.0  0.0   \n",
       "4   0.0         0.0      0.0  0.0     0.0    0.0       0.0    0.0  0.0   \n",
       "\n",
       "   adventure  ...  year  yes  yesterday   yo  yoga  york  young  youtube   yr  \\\n",
       "0        0.0  ...   0.0  0.0        0.0  0.0   0.0   0.0    0.0      0.0  0.0   \n",
       "1        0.0  ...   0.0  0.0        0.0  0.0   0.0   0.0    0.0      0.0  0.0   \n",
       "2        0.0  ...   0.0  0.0        0.0  0.0   0.0   0.0    0.0      0.0  0.0   \n",
       "3        0.0  ...   0.0  0.0        0.0  0.0   0.0   0.0    0.0      0.0  0.0   \n",
       "4        0.0  ...   0.0  0.0        0.0  0.0   0.0   0.0    0.0      0.0  0.0   \n",
       "\n",
       "   yummy  \n",
       "0    0.0  \n",
       "1    0.0  \n",
       "2    0.0  \n",
       "3    0.0  \n",
       "4    0.0  \n",
       "\n",
       "[5 rows x 1000 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Отобразим Bag-of-Words модель как DataFrame\n",
    "feature_names = tfidf_vectorizer.get_feature_names()\n",
    "lemmatized_tfidf = pd.DataFrame(bag_of_words_lemmatized.toarray(), columns=feature_names)\n",
    "lemmatized_tfidf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "geographic-powell",
   "metadata": {},
   "source": [
    "### Сравнение векторайзеров"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intellectual-banking",
   "metadata": {},
   "source": [
    "Проверьте ваши векторайзеры на корпусе который использовали на вебинаре, составьте таблицу метод векторизации и скор который вы получили (в методах векторизации по изменяйте параметры что бы добиться лучшего скора) обратите внимание как падает/растёт скор при уменьшении количества фичей, и изменении параметров, так же попробуйте применить к векторайзерам PCA для сокращения размерности посмотрите на качество сделайте выводы."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fluid-metadata",
   "metadata": {},
   "source": [
    "**Base score**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "loose-fifty",
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_df['label'] = combine_df['label'].apply(lambda y: 0 if np.isnan(y) else y)\n",
    "y = combine_df['label'].astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "resident-ozone",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.float_format = '{:.8f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "atmospheric-acquisition",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [['stemmed_count', stemmed_count], ['stemmed_tfidf', stemmed_tfidf], \n",
    "           ['lemmatized_count', lemmatized_count], ['lemmatized_tfidf', lemmatized_tfidf]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "floating-somewhere",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for feature in features:\n",
    "    X_train, X_test, y_train, y_test = model_selection.train_test_split(feature[1], y)\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    results.append([\n",
    "        feature[0], \n",
    "        mean_squared_error(y_test, y_pred), \n",
    "        mean_absolute_error(y_test, y_pred),\n",
    "        median_absolute_error(y_test, y_pred), \n",
    "        r2_score(y_test, y_pred)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bottom-binary",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>mse</th>\n",
       "      <th>mae</th>\n",
       "      <th>median</th>\n",
       "      <th>r2 score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>stemmed_count</td>\n",
       "      <td>0.03566267</td>\n",
       "      <td>0.08347376</td>\n",
       "      <td>0.03426095</td>\n",
       "      <td>0.17008151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>stemmed_tfidf</td>\n",
       "      <td>0.03501990</td>\n",
       "      <td>0.08205668</td>\n",
       "      <td>0.02947686</td>\n",
       "      <td>0.19747529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lemmatized_count</td>\n",
       "      <td>10615079800861263872.00000000</td>\n",
       "      <td>29389059.72624626</td>\n",
       "      <td>0.03780336</td>\n",
       "      <td>-257243161171426869248.00000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lemmatized_tfidf</td>\n",
       "      <td>1370113735014475008.00000000</td>\n",
       "      <td>10558503.59014027</td>\n",
       "      <td>0.03061897</td>\n",
       "      <td>-31186608244466642944.00000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              model                           mse               mae  \\\n",
       "0     stemmed_count                    0.03566267        0.08347376   \n",
       "1     stemmed_tfidf                    0.03501990        0.08205668   \n",
       "2  lemmatized_count 10615079800861263872.00000000 29389059.72624626   \n",
       "3  lemmatized_tfidf  1370113735014475008.00000000 10558503.59014027   \n",
       "\n",
       "      median                        r2 score  \n",
       "0 0.03426095                      0.17008151  \n",
       "1 0.02947686                      0.19747529  \n",
       "2 0.03780336 -257243161171426869248.00000000  \n",
       "3 0.03061897  -31186608244466642944.00000000  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_df = pd.DataFrame(results, columns=['model', 'mse', 'mae', 'median', 'r2 score'])\n",
    "score_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clear-bubble",
   "metadata": {},
   "source": [
    "**Уменьшение размерности**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "noted-twelve",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = [500, 3000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "organized-auction",
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in max_features:\n",
    "    \n",
    "    tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 2), \n",
    "                                   analyzer='word', \n",
    "                                   binary=False, \n",
    "                                   tokenizer=str.split, \n",
    "                                   stop_words='english', \n",
    "                                   max_df=0.9, \n",
    "                                   max_features=feature)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = model_selection.train_test_split(lemmatized_tfidf, y)\n",
    "\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    results.append([\n",
    "        f'{max_features} lemmatized tfidf',\n",
    "        mean_squared_error(y_test, y_pred), \n",
    "        mean_absolute_error(y_test, y_pred),\n",
    "        median_absolute_error(y_test, y_pred), \n",
    "        r2_score(y_test, y_pred)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "responsible-reminder",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>mse</th>\n",
       "      <th>mae</th>\n",
       "      <th>median</th>\n",
       "      <th>r2 score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>stemmed_count</td>\n",
       "      <td>0.03566267</td>\n",
       "      <td>0.08347376</td>\n",
       "      <td>0.03426095</td>\n",
       "      <td>0.17008151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>stemmed_tfidf</td>\n",
       "      <td>0.03501990</td>\n",
       "      <td>0.08205668</td>\n",
       "      <td>0.02947686</td>\n",
       "      <td>0.19747529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lemmatized_count</td>\n",
       "      <td>10615079800861263872.00000000</td>\n",
       "      <td>29389059.72624626</td>\n",
       "      <td>0.03780336</td>\n",
       "      <td>-257243161171426869248.00000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lemmatized_tfidf</td>\n",
       "      <td>1370113735014475008.00000000</td>\n",
       "      <td>10558503.59014027</td>\n",
       "      <td>0.03061897</td>\n",
       "      <td>-31186608244466642944.00000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[500, 3000] lemmatized tfidf</td>\n",
       "      <td>0.03617191</td>\n",
       "      <td>0.08231934</td>\n",
       "      <td>0.02893440</td>\n",
       "      <td>0.21857878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[500, 3000] lemmatized tfidf</td>\n",
       "      <td>28381430800180473856.00000000</td>\n",
       "      <td>58322493.56283243</td>\n",
       "      <td>0.02991418</td>\n",
       "      <td>-648200046439436124160.00000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          model                           mse  \\\n",
       "0                 stemmed_count                    0.03566267   \n",
       "1                 stemmed_tfidf                    0.03501990   \n",
       "2              lemmatized_count 10615079800861263872.00000000   \n",
       "3              lemmatized_tfidf  1370113735014475008.00000000   \n",
       "4  [500, 3000] lemmatized tfidf                    0.03617191   \n",
       "5  [500, 3000] lemmatized tfidf 28381430800180473856.00000000   \n",
       "\n",
       "                mae     median                        r2 score  \n",
       "0        0.08347376 0.03426095                      0.17008151  \n",
       "1        0.08205668 0.02947686                      0.19747529  \n",
       "2 29389059.72624626 0.03780336 -257243161171426869248.00000000  \n",
       "3 10558503.59014027 0.03061897  -31186608244466642944.00000000  \n",
       "4        0.08231934 0.02893440                      0.21857878  \n",
       "5 58322493.56283243 0.02991418 -648200046439436124160.00000000  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_df = pd.DataFrame(results, columns=['model', 'mse', 'mae', 'median', 'r2 score'])\n",
    "score_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "important-eleven",
   "metadata": {},
   "source": [
    "**PCA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "failing-deviation",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = [50, 300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "lesbian-excellence",
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in max_features:\n",
    "    pca = decomposition.PCA(n_components=feature)\n",
    "    pca.fit(lemmatized_tfidf)\n",
    "\n",
    "    lemmatized_tfidf_pca = pca.transform(lemmatized_tfidf)\n",
    "    X_train, X_test, y_train, y_test = model_selection.train_test_split(lemmatized_tfidf_pca, y)\n",
    "\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    results.append([\n",
    "        f'PCA {max_features} lemmatized tfidf',\n",
    "        mean_squared_error(y_test, y_pred), \n",
    "        mean_absolute_error(y_test, y_pred),\n",
    "        median_absolute_error(y_test, y_pred), \n",
    "        r2_score(y_test, y_pred)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aquatic-commerce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>mse</th>\n",
       "      <th>mae</th>\n",
       "      <th>median</th>\n",
       "      <th>r2 score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>stemmed_count</td>\n",
       "      <td>0.03566267</td>\n",
       "      <td>0.08347376</td>\n",
       "      <td>0.03426095</td>\n",
       "      <td>0.17008151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>stemmed_tfidf</td>\n",
       "      <td>0.03501990</td>\n",
       "      <td>0.08205668</td>\n",
       "      <td>0.02947686</td>\n",
       "      <td>0.19747529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lemmatized_count</td>\n",
       "      <td>10615079800861263872.00000000</td>\n",
       "      <td>29389059.72624626</td>\n",
       "      <td>0.03780336</td>\n",
       "      <td>-257243161171426869248.00000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lemmatized_tfidf</td>\n",
       "      <td>1370113735014475008.00000000</td>\n",
       "      <td>10558503.59014027</td>\n",
       "      <td>0.03061897</td>\n",
       "      <td>-31186608244466642944.00000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[500, 3000] lemmatized tfidf</td>\n",
       "      <td>0.03617191</td>\n",
       "      <td>0.08231934</td>\n",
       "      <td>0.02893440</td>\n",
       "      <td>0.21857878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[500, 3000] lemmatized tfidf</td>\n",
       "      <td>28381430800180473856.00000000</td>\n",
       "      <td>58322493.56283243</td>\n",
       "      <td>0.02991418</td>\n",
       "      <td>-648200046439436124160.00000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>PCA [50, 300] lemmatized tfidf</td>\n",
       "      <td>0.04080016</td>\n",
       "      <td>0.08850586</td>\n",
       "      <td>0.04956678</td>\n",
       "      <td>0.07596445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>PCA [50, 300] lemmatized tfidf</td>\n",
       "      <td>0.03721058</td>\n",
       "      <td>0.08298610</td>\n",
       "      <td>0.03160749</td>\n",
       "      <td>0.19486282</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            model                           mse  \\\n",
       "0                   stemmed_count                    0.03566267   \n",
       "1                   stemmed_tfidf                    0.03501990   \n",
       "2                lemmatized_count 10615079800861263872.00000000   \n",
       "3                lemmatized_tfidf  1370113735014475008.00000000   \n",
       "4    [500, 3000] lemmatized tfidf                    0.03617191   \n",
       "5    [500, 3000] lemmatized tfidf 28381430800180473856.00000000   \n",
       "6  PCA [50, 300] lemmatized tfidf                    0.04080016   \n",
       "7  PCA [50, 300] lemmatized tfidf                    0.03721058   \n",
       "\n",
       "                mae     median                        r2 score  \n",
       "0        0.08347376 0.03426095                      0.17008151  \n",
       "1        0.08205668 0.02947686                      0.19747529  \n",
       "2 29389059.72624626 0.03780336 -257243161171426869248.00000000  \n",
       "3 10558503.59014027 0.03061897  -31186608244466642944.00000000  \n",
       "4        0.08231934 0.02893440                      0.21857878  \n",
       "5 58322493.56283243 0.02991418 -648200046439436124160.00000000  \n",
       "6        0.08850586 0.04956678                      0.07596445  \n",
       "7        0.08298610 0.03160749                      0.19486282  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_df = pd.DataFrame(results, columns=['model', 'mse', 'mae', 'median', 'r2 score'])\n",
    "score_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grand-horror",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Содержание",
   "title_sidebar": "Содержание",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
